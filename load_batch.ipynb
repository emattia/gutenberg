{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddie/micromamba/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 21:41:26 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 21:41:26,471\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from eval_utils import display_prompt, display_responses\n",
    "from load_torchtune_ds import load_gutenberg_dataset\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtune.dev.grpo.data import padded_collate_rl\n",
    "from torchtune import config\n",
    "from torchtune.config._utils import _get_component_from_path\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 1\n",
    "rank = 1\n",
    "batch_size = 2\n",
    "grpo_size = 2\n",
    "\n",
    "### NOTE: this is what we are replacing.\n",
    "# cfg_dataset = DictConfig({\n",
    "#     '_component_': 'torchtune.dev.grpo.gsm8k.gsm8k_dataset',\n",
    "#     'partition': '3-5/100' \n",
    "# })\n",
    "\n",
    "root_path = os.path.expanduser('~/dev/nebius-experiments/projects/torchtune/trained_models/')\n",
    "\n",
    "cfg_tokenizer = DictConfig({\n",
    "    '_component_': 'torchtune.models.llama3.llama3_tokenizer',\n",
    "    'path': os.path.join(root_path, 'Llama3_3_70B_GRPOd_gsm8k_default_reward/original/tokenizer.model'),\n",
    "    'max_seq_len': 'null'\n",
    "})\n",
    "collate_fn = 'torchtune.dev.grpo.data.padded_collate_rl'\n",
    "\n",
    "tokenizer = config.instantiate(cfg_tokenizer)\n",
    "collate_fn = _get_component_from_path(collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 144 passages from 144 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 144/144 [00:00<00:00, 33506.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset for historical context reasoning\n",
    "data_path = os.path.join(os.getcwd(), \"gutenberg_dataset\")\n",
    "if not os.path.exists(data_path):\n",
    "    raise ValueError(\"Did you run the download.py script?\")\n",
    "\n",
    "dataset = load_gutenberg_dataset(tokenizer, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: padded_collate_rl(\n",
    "        batch,\n",
    "        padding_idx=tokenizer.pad_id,\n",
    "        ignore_idx=-100,  # CROSS_ENTROPY_IGNORE_IDX\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[128000,     32,  21765,  ...,  22103,     25,    220],\n",
       "         [128000,     32,  21765,  ..., 128004, 128004, 128004]]),\n",
       " 'answers': ['enlightenment (1725)', 'enlightenment (1725)']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(dataloader._get_iterator())\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dataloader._get_iterator())\n",
    "tokens = batch[\"tokens\"]         # tokenized prompts\n",
    "answers = batch[\"answers\"]       # untokenized answers\n",
    "tokens = tokens                  # [batch_size x num_tokens_per_prompt]\n",
    "tokens_ls = tokens.tolist()\n",
    "out = []\n",
    "_prompts = []\n",
    "_answers = []\n",
    "for i in range(tokens.shape[0]):\n",
    "    prompt = tokenizer.decode(tokens_ls[i])\n",
    "    _prompts.extend([prompt] * grpo_size) \n",
    "    answer = answers[i]\n",
    "    _answers.extend([answer] * grpo_size)\n",
    "    # display(HTML(display_prompt(\n",
    "    #     prompt, \n",
    "    #     answer, \n",
    "    #     tokenizer\n",
    "    # )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torchtune.modules.transforms.tokenizers import ModelTokenizer\n",
    "import re\n",
    "\n",
    "def extract_tags(text: str) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Parse XML-like tags from text. Returns a dictionary with keys 'think', 'answer_era', and 'answer_date'.\n",
    "    The values are lists of strings, with each string being the content of a tag.\n",
    "    \"\"\"\n",
    "    xml_string = f\"<root>{text}</root>\"\n",
    "    try:\n",
    "        root = ET.fromstring(xml_string)\n",
    "        return {\n",
    "            \"think\": [elem.text if elem.text is not None else \"\" for elem in root.findall(\"think\")],\n",
    "            \"answer_era\": [elem.text if elem.text is not None else \"\" for elem in root.findall(\"answer_era\")],\n",
    "            \"answer_date\": [elem.text if elem.text is not None else \"\" for elem in root.findall(\"answer_date\")]\n",
    "        }\n",
    "    except ET.ParseError:\n",
    "        return {\"think\": [], \"answer_era\": [], \"answer_date\": []}\n",
    "\n",
    "def shaped_correctness_reward(answer: str, completion: str) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Reward function for verifiable rewards with shaping for era and date identification.\n",
    "\n",
    "    Args:\n",
    "        answer (str): Ground-truth answer in the format \"era (date)\"\n",
    "        completion (str): Model's completion\n",
    "    Returns:\n",
    "        reward: (float) A shaped reward for format and correctness\n",
    "        success: (float) A binary measure of success (1 if fully successful, 0 otherwise)\n",
    "    \"\"\"\n",
    "    reward = 0.0\n",
    "    success = 0.0\n",
    "    \n",
    "    # Parse the ground truth era and date\n",
    "    gt_match = re.match(r'([a-z]+)\\s*\\((\\d+)\\)', answer.lower())\n",
    "    if gt_match:\n",
    "        gt_era = gt_match.group(1)\n",
    "        gt_date = gt_match.group(2)\n",
    "    else:\n",
    "        # Fallback if parsing fails\n",
    "        gt_era = answer\n",
    "        gt_date = \"\"\n",
    "    \n",
    "    # Extract tags from completion\n",
    "    tags = extract_tags(completion)\n",
    "    \n",
    "    # Format rewards - consistent with the original example\n",
    "    if len(tags[\"think\"]) == 1:\n",
    "        reward += 5.0  # Reward for having a thinking section\n",
    "    \n",
    "    if len(tags[\"answer_era\"]) == 1:\n",
    "        reward += 2.5  # Reward for having an era answer tag\n",
    "    \n",
    "    if len(tags[\"answer_date\"]) == 1:\n",
    "        reward += 2.5  # Reward for having a date answer tag\n",
    "    \n",
    "    # Correctness rewards for era\n",
    "    if tags[\"answer_era\"] and any(gt_era == attempt.lower() for attempt in tags[\"answer_era\"]):\n",
    "        # One of the answer_era tags has the exact right era\n",
    "        reward += 20.0\n",
    "    elif tags[\"answer_era\"] and any(gt_era in attempt.lower() for attempt in tags[\"answer_era\"]):\n",
    "        # One of the answer_era tags contains the right era as a substring\n",
    "        reward += 10.0\n",
    "    \n",
    "    # Correctness rewards for date\n",
    "    if gt_date and tags[\"answer_date\"]:\n",
    "        try:\n",
    "            gt_year = int(gt_date)\n",
    "            for attempt in tags[\"answer_date\"]:\n",
    "                if attempt.isdigit():\n",
    "                    attempt_year = int(attempt)\n",
    "                    year_diff = abs(gt_year - attempt_year)\n",
    "                    \n",
    "                    if year_diff == 0:\n",
    "                        # Exact date match\n",
    "                        reward += 20.0\n",
    "                        break\n",
    "                    elif year_diff <= 20:\n",
    "                        # Within 20 years\n",
    "                        reward += 15.0\n",
    "                        break\n",
    "                    elif year_diff <= 50:\n",
    "                        # Within 50 years\n",
    "                        reward += 10.0\n",
    "                        break\n",
    "        except ValueError:\n",
    "            pass  # Ignore non-numeric date values\n",
    "    \n",
    "    # Full success - both era and date are correct\n",
    "    if tags[\"answer_era\"] and tags[\"answer_date\"] and \\\n",
    "       len(tags[\"answer_era\"]) > 0 and tags[\"answer_era\"][-1].lower() == gt_era and \\\n",
    "       len(tags[\"answer_date\"]) > 0 and tags[\"answer_date\"][-1].isdigit() and abs(int(tags[\"answer_date\"][-1]) - int(gt_date)) <= 20:\n",
    "        reward = 100.0\n",
    "        success = 1.0\n",
    "    \n",
    "    return reward, success\n",
    "\n",
    "def batch_shaped_correctness_reward(\n",
    "    tokenizer: ModelTokenizer, completions: torch.Tensor, answers: list[str]\n",
    ") -> Tuple[torch.Tensor]:\n",
    "    \"\"\"Utility function to apply the shaped reward function to a GRPO-style batch of completions.\"\"\"\n",
    "\n",
    "    batch_size, grpo_size, *_ = completions.shape\n",
    "    rewards = torch.zeros(batch_size, grpo_size, dtype=torch.float32)\n",
    "    successes = torch.zeros(batch_size, grpo_size, dtype=torch.float32)\n",
    "    # completions :: [B, G, L]\n",
    "    for b in range(batch_size):\n",
    "        for g in range(grpo_size):\n",
    "            text_completion = tokenizer.decode(\n",
    "                completions[b, g].tolist()\n",
    "            )  # skips special tokens, stops at eos\n",
    "            reward, success = shaped_correctness_reward(\n",
    "                answer=answers[b], completion=text_completion\n",
    "            )\n",
    "            rewards[b, g] = reward\n",
    "            successes[b, g] = success\n",
    "\n",
    "    return rewards, successes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-22 21:41:31 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 03-22 21:41:31 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 03-22 21:41:31 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/tmp/Llama-3.2-3B-Instruct/', speculative_config=None, tokenizer='/tmp/Llama-3.2-3B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/tmp/Llama-3.2-3B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-22 21:41:32 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 03-22 21:41:32 model_runner.py:1110] Starting to load model /tmp/Llama-3.2-3B-Instruct/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.86it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.72it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 21:41:34 model_runner.py:1115] Loading model weights took 6.0160 GB\n",
      "INFO 03-22 21:41:35 worker.py:267] Memory profiling takes 0.50 seconds\n",
      "INFO 03-22 21:41:35 worker.py:267] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.90) = 71.19GiB\n",
      "INFO 03-22 21:41:35 worker.py:267] model weights take 6.02GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 63.81GiB.\n",
      "INFO 03-22 21:41:35 executor_base.py:110] # CUDA blocks: 37339, # CPU blocks: 2340\n",
      "INFO 03-22 21:41:35 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 4.56x\n",
      "INFO 03-22 21:41:37 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:10<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-22 21:41:47 model_runner.py:1562] Graph capturing finished in 10 secs, took 0.29 GiB\n",
      "INFO 03-22 21:41:47 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 13.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = '/tmp/Llama-3.2-3B-Instruct/'\n",
    "llm = LLM(\n",
    "    model=path, \n",
    "    task=\"generate\", \n",
    "    trust_remote_code=True,\n",
    "    # tensor_parallel_size=1,\n",
    "    dtype='bfloat16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s, est. speed input: 843.85 toks/s, output: 726.39 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8, \n",
    "    top_p=0.95,\n",
    "    max_tokens=512\n",
    ")\n",
    "output = llm.generate(_prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_ids = [\n",
    "    128001,\n",
    "    128009,\n",
    "    128008\n",
    "]\n",
    "pad_id = 128004\n",
    "max_tokens = 512\n",
    "\n",
    "data = []\n",
    "for o in output:\n",
    "    out_tokens = list(o.outputs[0].token_ids)\n",
    "    if len(out_tokens) < max_tokens:\n",
    "        out_tokens += [pad_id] * (max_tokens - len(out_tokens))\n",
    "    data.append(out_tokens)\n",
    "responses=torch.tensor(data, dtype=torch.int32).reshape(batch_size, grpo_size, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses.shape # [batch_size, grpo_size, generation_max_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, successes = batch_shaped_correctness_reward(\n",
    "    tokenizer=tokenizer, \n",
    "    completions=responses, \n",
    "    answers=_answers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100., 100.],\n",
       "        [  0.,   0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages = (rewards - rewards.mean(1, keepdim=True)) / (\n",
    "    rewards.std(1, keepdim=True) + 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m display(HTML(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mdisplay_responses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrpo_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m=\u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuccesses\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuccesses\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m ))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gutenberg-data-processing/eval_utils.py:269\u001b[39m, in \u001b[36mdisplay_responses\u001b[39m\u001b[34m(responses, tokenizer, grpo_size, advantages, rewards, successes, component_details, show_n)\u001b[39m\n\u001b[32m    267\u001b[39m html_output += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<div>Response #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m</div>\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_rewards:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     reward = \u001b[43mget_item_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m     success = get_item_value(successes, i)\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m success > \u001b[32m0.5\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/gutenberg-data-processing/eval_utils.py:86\u001b[39m, in \u001b[36mdisplay_responses.<locals>.get_item_value\u001b[39m\u001b[34m(tensor, index)\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor[index].item()\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# Handle 2D tensor [batch_size, grpo_size]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m.item()\n",
      "\u001b[31mIndexError\u001b[39m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "display(HTML(\n",
    "    display_responses(\n",
    "        responses,\n",
    "        tokenizer, \n",
    "        grpo_size, \n",
    "        advantages=advantages, \n",
    "        rewards=rewards, \n",
    "        successes=successes\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
