{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 3rd party dependencies\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtune.dev.grpo.data import padded_collate_rl\n",
    "from torchtune import config\n",
    "from torchtune.config._utils import _get_component_from_path\n",
    "from vllm import LLM, SamplingParams\n",
    "from omegaconf import DictConfig\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Custom dependencies\n",
    "from viz import display_prompt, display_responses\n",
    "from load_torchtune_ds import load_gutenberg_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 1\n",
    "rank = 1\n",
    "batch_size = 2\n",
    "grpo_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize tokenizer and configure model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_chkpt_path = os.path.expanduser('~/dev/nebius-experiments/projects/torchtune/trained_models/')\n",
    "# model_path = '/tmp/Llama-3.2-3B-Instruct/'\n",
    "# model_path = '/tmp/checkpoints/llama3_2_3B_grpo_gutenberg/epoch_9'\n",
    "model_path = '/tmp/checkpoints/llama3_2_3B_grpo_gutenberg_rewards_v1/epoch_9'\n",
    "cfg_tokenizer = DictConfig({\n",
    "    '_component_': 'torchtune.models.llama3.llama3_tokenizer',\n",
    "    #'path': os.path.join(root_chkpt_path, model_path, 'original/tokenizer.model'),\n",
    "    'path': os.path.join(model_path, 'original/tokenizer.model'),\n",
    "    'max_seq_len': 'null'\n",
    "})\n",
    "collate_fn = 'torchtune.dev.grpo.data.padded_collate_rl'\n",
    "tokenizer = config.instantiate(cfg_tokenizer)\n",
    "collate_fn = _get_component_from_path(collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this is what we are replacing from og GRPO scripts.\n",
    "# cfg_dataset = DictConfig({\n",
    "#     '_component_': 'torchtune.dev.grpo.gsm8k.gsm8k_dataset',\n",
    "#     'partition': '3-5/100' \n",
    "# })\n",
    "data_path = os.path.join(os.getcwd(), \"gutenberg_dataset\", \"validation\")\n",
    "if not os.path.exists(data_path):\n",
    "    raise ValueError(\"Did you run the download.py script?\")\n",
    "dataset = load_gutenberg_dataset(tokenizer, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: padded_collate_rl(\n",
    "        batch,\n",
    "        padding_idx=tokenizer.pad_id,\n",
    "        ignore_idx=-100,  # CROSS_ENTROPY_IGNORE_IDX\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample batch for inspection/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dataloader._get_iterator())\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dataloader._get_iterator())\n",
    "tokens = batch[\"tokens\"]         # tokenized prompts\n",
    "answers = batch[\"answers\"]       # untokenized answers\n",
    "tokens = tokens                  # [batch_size x num_tokens_per_prompt]\n",
    "tokens_ls = tokens.tolist()\n",
    "out = []\n",
    "_prompts = []\n",
    "_answers = []\n",
    "for i in range(tokens.shape[0]):\n",
    "    prompt = tokenizer.decode(tokens_ls[i])\n",
    "    _prompts.extend([prompt] * grpo_size) \n",
    "    answer = answers[i]\n",
    "    _answers.extend([answer] * grpo_size)\n",
    "\n",
    "    ## Uncomment below to view the prompt inline.\n",
    "    # display(HTML(display_prompt(\n",
    "    #     prompt, \n",
    "    #     answer, \n",
    "    #     tokenizer\n",
    "    # )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=model_path, \n",
    "    task=\"generate\", \n",
    "    trust_remote_code=True,\n",
    "    # tensor_parallel_size=1,\n",
    "    dtype='bfloat16'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass / inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per inference call hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 512 # reused later\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8, \n",
    "    top_p=0.95,\n",
    "    max_tokens=max_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.generate(_prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Unique to the model/tokenizer\n",
    "# This specific configuration is for meta-llama tokenizers.\n",
    "stop_token_ids = [\n",
    "    128001,\n",
    "    128009,\n",
    "    128008\n",
    "]\n",
    "pad_id = 128004\n",
    "\n",
    "data = []\n",
    "for o in output:\n",
    "    out_tokens = list(o.outputs[0].token_ids)\n",
    "    if len(out_tokens) < max_tokens:\n",
    "        out_tokens += [pad_id] * (max_tokens - len(out_tokens))\n",
    "    data.append(out_tokens)\n",
    "responses=torch.tensor(data, dtype=torch.int32).reshape(batch_size, grpo_size, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.shape # [batch_size, grpo_size, generation_max_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define reward function\n",
    "\n",
    "For reward functions we adopt the following workflow. \n",
    "\n",
    "First, define a Python module in the structure `rewards_{id}.py`. The file should have a class called `RewardServer`. The `RewardServer` class should have a method called `batch_shaped_correctness_reward`, with this signature:\n",
    "```python\n",
    "def batch_shaped_correctness_reward(\n",
    "        self,\n",
    "        tokenizer: ModelTokenizer, \n",
    "        completions: torch.Tensor, \n",
    "        answers: List[str],\n",
    "        details_report: bool = False\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[List[Dict[str, Any]]]]\n",
    "```\n",
    "\n",
    "To use this reward function during training, view the torchtune config file (e.g., `3B_full_grpo_llama_32.yaml`), and set the `reward_fn: {id}` field. For details, see the `grpo_full_finetune_distributed.py` function in this repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rewards_v0.py\n",
    "\n",
    "import re\n",
    "import html\n",
    "from xml.etree import ElementTree as ET\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "from torchtune.modules.transforms.tokenizers import ModelTokenizer\n",
    "\n",
    "# TODO: Scoop this from download_src_data module to ensure consistency.\n",
    "VALID_ERAS = [\n",
    "    \"renaissance\",\n",
    "    \"enlightenment\",\n",
    "    \"victorian\",\n",
    "    \"edwardian\",\n",
    "    \"modern\"\n",
    "]\n",
    "\n",
    "class RewardServer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def check_outside_text(self, text: str) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        More robust function to detect text outside the required tags.\n",
    "\n",
    "        Args:\n",
    "            text: The text to analyze\n",
    "\n",
    "        Returns:\n",
    "            tuple: (has_outside_text, outside_text)\n",
    "        \"\"\"\n",
    "        # First strip whitespace\n",
    "        text = text.strip()\n",
    "\n",
    "        # Use regex to extract all tag content\n",
    "        pattern = r\"<(think|answer_date|answer_era)>(.*?)</\\1>\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "        # Create a cleaned version for comparison\n",
    "        cleaned_text = text\n",
    "\n",
    "        # Remove all valid tag content\n",
    "        for tag, content in matches:\n",
    "            cleaned_text = cleaned_text.replace(f\"<{tag}>{content}</{tag}>\", \"\", 1)\n",
    "\n",
    "        # Strip whitespace again\n",
    "        cleaned_text = cleaned_text.strip()\n",
    "\n",
    "        return bool(cleaned_text), cleaned_text\n",
    "\n",
    "\n",
    "    def extract_tags(self, text: str) -> dict[str, list[str]]:\n",
    "        \"\"\"\n",
    "        Parse XML-like tags from text, with improved handling for malformed XML.\n",
    "\n",
    "        Args:\n",
    "            text: Text potentially containing XML tags\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with tag content keyed by tag name\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            \"think\": [],\n",
    "            \"answer_era\": [],\n",
    "            \"answer_date\": []\n",
    "        }\n",
    "\n",
    "        # First, try regex method (more robust for malformed XML)\n",
    "        for tag in [\"think\", \"answer_era\", \"answer_date\"]:\n",
    "            pattern = f\"<{tag}>(.*?)</{tag}>\"\n",
    "            matches = re.findall(pattern, text, re.DOTALL)\n",
    "            if matches:\n",
    "                # Trim whitespace from each match\n",
    "                results[tag] = [match.strip() for match in matches]\n",
    "\n",
    "        # If regex found matches, return those\n",
    "        if any(len(v) > 0 for v in results.values()):\n",
    "            return results\n",
    "\n",
    "        # Otherwise try the XML parser as fallback\n",
    "        xml_string = f\"<root>{text}</root>\"\n",
    "        try:\n",
    "            root = ET.fromstring(xml_string)\n",
    "            for tag in [\"think\", \"answer_era\", \"answer_date\"]:\n",
    "                results[tag] = [\n",
    "                    (elem.text.strip() if elem.text else \"\") \n",
    "                    for elem in root.findall(tag)\n",
    "                ]\n",
    "            return results\n",
    "        except ET.ParseError:\n",
    "            # Return the empty results if both methods fail\n",
    "            return results\n",
    "\n",
    "\n",
    "    def shaped_correctness_reward(self, answer: str, completion: str) -> tuple[float, float, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Reward function for gutenberg_eras_tour task with detailed diagnostics.\n",
    "\n",
    "        Args:\n",
    "            answer: Ground-truth answer string in format \"era (date)\"\n",
    "            completion: Model's unparsed completion string\n",
    "\n",
    "        Returns:\n",
    "            tuple: (reward_score, success_flag, details_dict)\n",
    "        \"\"\"\n",
    "\n",
    "        # Rewards can be negative. Max reward is 100.\n",
    "        # TODO: I've seen some folks speculating rewards in [0, 1] is beneficial.\n",
    "        reward = 0.0\n",
    "        success = 0.0\n",
    "\n",
    "        # Storage for diagnostics.\n",
    "        details = {\n",
    "            \"ground_truth\": {\n",
    "                \"original\": answer,\n",
    "                \"era\": \"\",\n",
    "                \"date\": \"\"\n",
    "            },\n",
    "            \"completion\": completion[:100] + (\"...\" if len(completion) > 100 else \"\"),  # Truncated for readability\n",
    "            \"extracted_tags\": {},\n",
    "            \"format_analysis\": {},\n",
    "            \"content_analysis\": {},\n",
    "            \"reward_components\": [],\n",
    "            \"total_reward\": 0.0,\n",
    "            \"success\": 0.0\n",
    "        }\n",
    "\n",
    "        # Parse the true labels we want the LLM to learn to infer based on its reasoning.\n",
    "        gt_match = re.match(r'([a-z]+)\\s*\\((\\d+)\\)', answer.lower())\n",
    "        if gt_match:\n",
    "            gt_era = gt_match.group(1).strip()\n",
    "            gt_date = gt_match.group(2).strip()\n",
    "        else:\n",
    "            # Fallback if parsing fails\n",
    "            gt_era = answer.lower().strip()\n",
    "            gt_date = \"\"\n",
    "        details[\"ground_truth\"][\"era\"] = gt_era\n",
    "        details[\"ground_truth\"][\"date\"] = gt_date\n",
    "\n",
    "        # Parse content from the LLM's completion.\n",
    "        tags = self.extract_tags(completion)\n",
    "        details[\"extracted_tags\"] = {\n",
    "            \"think\": tags[\"think\"],\n",
    "            \"answer_era\": tags[\"answer_era\"],\n",
    "            \"answer_date\": tags[\"answer_date\"]\n",
    "        }\n",
    "\n",
    "        ### FORMATTING PENALTY ### \n",
    "\n",
    "        # Max reward: 0.\n",
    "        # Min reward: -30.\n",
    "        # Does text exist outside of desired XML format?\n",
    "        has_outside_text, outside_text = self.check_outside_text(completion)\n",
    "        details[\"format_analysis\"][\"has_outside_text\"] = has_outside_text\n",
    "        if has_outside_text:\n",
    "            details[\"format_analysis\"][\"outside_text\"] = outside_text\n",
    "        # Apply significant penalties for text outside tags\n",
    "        if has_outside_text:\n",
    "            # More aggressive penalty for text outside tags\n",
    "            penalty = min(30.0, len(outside_text) * 0.2)\n",
    "            reward -= penalty\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"outside_text_penalty\",\n",
    "                \"value\": -penalty,\n",
    "                \"reason\": f\"Text found outside required tags: '{outside_text[:30]}...' ({len(outside_text)} chars)\"\n",
    "            })\n",
    "\n",
    "        ### FORMATTING REWARDS ###\n",
    "\n",
    "        ## <think> FORMATTING ##\n",
    "        # Max reward: 10.\n",
    "        # Min reward: -5.\n",
    "        if len(tags[\"think\"]) == 1:\n",
    "            reward += 10.0  # Good reward for having exactly one thinking section\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"think_tag_format\",\n",
    "                \"value\": 10.0,\n",
    "                \"reason\": \"Correct: Exactly one <think> tag\"\n",
    "            })\n",
    "        elif len(tags[\"think\"]) > 1:\n",
    "            reward += 2.5   # Smaller reward for having thinking, but too many sections\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"think_tag_format\",\n",
    "                \"value\": 2.5,\n",
    "                \"reason\": f\"Partial: {len(tags['think'])} <think> tags found (expected 1)\"\n",
    "            })\n",
    "        else:\n",
    "            reward -= 5.0   # Penalty for missing thinking section\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"think_tag_format\",\n",
    "                \"value\": -5.0,\n",
    "                \"reason\": \"Missing <think> tag\"\n",
    "            })\n",
    "\n",
    "        ## <answer_era> FORMATTING ##\n",
    "        # Max reward: 10.\n",
    "        # Min reward: -5.\n",
    "        if len(tags[\"answer_era\"]) == 1:\n",
    "            reward += 10.0  # Increased reward for having exactly one era tag\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"era_tag_format\",\n",
    "                \"value\": 10.0,\n",
    "                \"reason\": \"Correct: Exactly one <answer_era> tag\"\n",
    "            })\n",
    "        elif len(tags[\"answer_era\"]) > 1:\n",
    "            reward += 2.5   # Some reward for having era tags, but too many\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"era_tag_format\",\n",
    "                \"value\": 2.5,\n",
    "                \"reason\": f\"Partial: {len(tags['answer_era'])} <answer_era> tags found (expected 1)\"\n",
    "            })\n",
    "        else:\n",
    "            reward -= 10.0  # Stronger penalty for missing era\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"era_tag_format\",\n",
    "                \"value\": -10.0,\n",
    "                \"reason\": \"Missing <answer_era> tag\"\n",
    "            })\n",
    "            \n",
    "        ## <answer_date> FORMATTING ##\n",
    "        # Max reward: 10.\n",
    "        # Min reward: -5.\n",
    "        if len(tags[\"answer_date\"]) == 1:\n",
    "            reward += 10.0  # Increased reward for having exactly one date tag\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"date_tag_format\",\n",
    "                \"value\": 10.0,\n",
    "                \"reason\": \"Correct: Exactly one <answer_date> tag\"\n",
    "            })\n",
    "        elif len(tags[\"answer_date\"]) > 1:\n",
    "            reward += 2.5   # Some reward for having date tags, but too many\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"date_tag_format\",\n",
    "                \"value\": 2.5,\n",
    "                \"reason\": f\"Partial: {len(tags['answer_date'])} <answer_date> tags found (expected 1)\"\n",
    "            })\n",
    "        else:\n",
    "            reward -= 10.0  # Stronger penalty for missing date\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"date_tag_format\",\n",
    "                \"value\": -10.0,\n",
    "                \"reason\": \"Missing <answer_date> tag\"\n",
    "            })\n",
    "\n",
    "        ## <answer_era> validation rewards ##\n",
    "        # Max reward: 5.\n",
    "        # Min reward: -5.\n",
    "        if tags[\"answer_era\"]:\n",
    "            details[\"content_analysis\"][\"era\"] = {\n",
    "                \"provided\": [era.lower() for era in tags[\"answer_era\"]],\n",
    "                \"valid_eras\": VALID_ERAS,\n",
    "                \"ground_truth\": gt_era\n",
    "            }\n",
    "\n",
    "            # Is LLM-provided era in the valid list?\n",
    "            valid_provided = [era.lower() for era in tags[\"answer_era\"] if era.lower() in VALID_ERAS]\n",
    "            if valid_provided:\n",
    "                reward += 5.0  # Bonus for using a valid era from the list\n",
    "                details[\"reward_components\"].append({\n",
    "                    \"component\": \"era_validation\",\n",
    "                    \"value\": 5.0,\n",
    "                    \"reason\": f\"Used valid era(s): {', '.join(valid_provided)}\"\n",
    "                })\n",
    "            else:\n",
    "                reward -= 5.0  # Penalty for using invalid era\n",
    "                details[\"reward_components\"].append({\n",
    "                    \"component\": \"era_validation\",\n",
    "                    \"value\": -5.0,\n",
    "                    \"reason\": f\"Invalid era(s): {', '.join([era.lower() for era in tags['answer_era']])}\"\n",
    "                })\n",
    "\n",
    "        ## <answer_era> correctness rewards ##\n",
    "        # Max reward: 30.\n",
    "        # Min reward: 0.\n",
    "        if tags[\"answer_era\"]:\n",
    "            exact_match = any(gt_era == attempt.lower().strip() for attempt in tags[\"answer_era\"])\n",
    "            partial_match = any(gt_era in attempt.lower().strip() for attempt in tags[\"answer_era\"])\n",
    "\n",
    "            details[\"content_analysis\"][\"era_match\"] = {\n",
    "                \"exact_match\": exact_match,\n",
    "                \"partial_match\": partial_match\n",
    "            }\n",
    "\n",
    "            if exact_match:\n",
    "                # One of the answer_era tags has the exact right era\n",
    "                reward += 30.0  # Increased reward for correct era\n",
    "                details[\"reward_components\"].append({\n",
    "                    \"component\": \"era_correctness\",\n",
    "                    \"value\": 30.0,\n",
    "                    \"reason\": f\"Correct era: {gt_era}\"\n",
    "                })\n",
    "            elif partial_match:\n",
    "                # One of the answer_era tags contains the right era as a substring\n",
    "                reward += 10.0  # Partial reward\n",
    "                details[\"reward_components\"].append({\n",
    "                    \"component\": \"era_correctness\",\n",
    "                    \"value\": 10.0,\n",
    "                    \"reason\": f\"Partial era match: Contains '{gt_era}'\"\n",
    "                })\n",
    "            else:\n",
    "                details[\"reward_components\"].append({\n",
    "                    \"component\": \"era_correctness\",\n",
    "                    \"value\": 0.0,\n",
    "                    \"reason\": f\"Incorrect era: Expected '{gt_era}'\"\n",
    "                })\n",
    "\n",
    "        ## <answer_date> correctness rewards ##\n",
    "        # Max reward: 30.\n",
    "        # Min reward: -5.\n",
    "        if gt_date and tags[\"answer_date\"]:\n",
    "            try:\n",
    "                gt_year = int(gt_date)\n",
    "\n",
    "                # Parse all dates and track errors\n",
    "                date_attempts = []\n",
    "                valid_dates = []\n",
    "\n",
    "                for attempt in tags[\"answer_date\"]:\n",
    "                    attempt = attempt.strip()\n",
    "                    date_attempts.append(attempt)\n",
    "                    if attempt.isdigit():\n",
    "                        valid_dates.append(int(attempt))\n",
    "\n",
    "                details[\"content_analysis\"][\"date\"] = {\n",
    "                    \"provided\": date_attempts,\n",
    "                    \"valid_dates\": valid_dates,\n",
    "                    \"ground_truth\": gt_year\n",
    "                }\n",
    "\n",
    "                if valid_dates:\n",
    "                    # Find best date attempt (closest to ground truth)\n",
    "                    best_diff = min(abs(date - gt_year) for date in valid_dates)\n",
    "                    best_date = next(date for date in valid_dates if abs(date - gt_year) == best_diff)\n",
    "\n",
    "                    details[\"content_analysis\"][\"date\"][\"best_match\"] = {\n",
    "                        \"value\": best_date,\n",
    "                        \"difference\": best_diff\n",
    "                    }\n",
    "\n",
    "                    # Award based on closest date\n",
    "                    if best_diff == 0:\n",
    "                        # Exact date match\n",
    "                        reward += 30.0  # Increased reward\n",
    "                        details[\"reward_components\"].append({\n",
    "                            \"component\": \"date_correctness\",\n",
    "                            \"value\": 30.0,\n",
    "                            \"reason\": f\"Exact date match: {best_date}\"\n",
    "                        })\n",
    "                    elif best_diff <= 20:\n",
    "                        # Within 20 years\n",
    "                        reward += 20.0  # Increased reward\n",
    "                        details[\"reward_components\"].append({\n",
    "                            \"component\": \"date_correctness\",\n",
    "                            \"value\": 20.0,\n",
    "                            \"reason\": f\"Close date match: {best_date} (within 20 years of {gt_year})\"\n",
    "                        })\n",
    "                    elif best_diff <= 50:\n",
    "                        # Within 50 years\n",
    "                        reward += 10.0\n",
    "                        details[\"reward_components\"].append({\n",
    "                            \"component\": \"date_correctness\",\n",
    "                            \"value\": 10.0,\n",
    "                            \"reason\": f\"Approximate date: {best_date} (within 50 years of {gt_year})\"\n",
    "                        })\n",
    "                    elif best_diff <= 100:\n",
    "                        # Within 100 years\n",
    "                        reward += 5.0\n",
    "                        details[\"reward_components\"].append({\n",
    "                            \"component\": \"date_correctness\",\n",
    "                            \"value\": 5.0,\n",
    "                            \"reason\": f\"Distant date: {best_date} (within 100 years of {gt_year})\"\n",
    "                        })\n",
    "                    else:\n",
    "                        # More than 100 years off\n",
    "                        reward -= 5.0  # Small penalty for very wrong date\n",
    "                        details[\"reward_components\"].append({\n",
    "                            \"component\": \"date_correctness\",\n",
    "                            \"value\": -5.0,\n",
    "                            \"reason\": f\"Incorrect date: {best_date} (more than 100 years from {gt_year})\"\n",
    "                        })\n",
    "                else:\n",
    "                    # No valid numeric dates found\n",
    "                    reward -= 5.0  # Penalty for non-numeric date\n",
    "                    details[\"reward_components\"].append({\n",
    "                        \"component\": \"date_correctness\",\n",
    "                        \"value\": -5.0,\n",
    "                        \"reason\": f\"Non-numeric date(s): {', '.join(date_attempts)}\"\n",
    "                    })\n",
    "\n",
    "            except ValueError as e:\n",
    "                # Penalty for non-numeric date\n",
    "                reward -= 5.0\n",
    "                details[\"reward_components\"].append({\n",
    "                    \"component\": \"date_correctness\",\n",
    "                    \"value\": -5.0,\n",
    "                    \"reason\": f\"Date parsing error: {str(e)}\"\n",
    "                })\n",
    "\n",
    "        ## Success criteria ##\n",
    "        # Both era and date must be correct AND format must be perfect\n",
    "        perfect_format = (\n",
    "            len(tags[\"think\"]) == 1 and \n",
    "            len(tags[\"answer_era\"]) == 1 and \n",
    "            len(tags[\"answer_date\"]) == 1 and\n",
    "            not has_outside_text\n",
    "        )\n",
    "\n",
    "        correct_era = (\n",
    "            tags[\"answer_era\"] and \n",
    "            tags[\"answer_era\"][0].lower().strip() == gt_era\n",
    "        )\n",
    "\n",
    "        correct_date = (\n",
    "            gt_date and\n",
    "            tags[\"answer_date\"] and\n",
    "            tags[\"answer_date\"][0].isdigit() and \n",
    "            abs(int(tags[\"answer_date\"][0]) - int(gt_date)) <= 20\n",
    "        )\n",
    "\n",
    "        details[\"success_criteria\"] = {\n",
    "            \"perfect_format\": perfect_format,\n",
    "            \"correct_era\": correct_era,\n",
    "            \"correct_date\": correct_date\n",
    "        }\n",
    "\n",
    "        if perfect_format and correct_era and correct_date:\n",
    "            reward = 100.0\n",
    "            success = 1.0\n",
    "            details[\"reward_components\"].append({\n",
    "                \"component\": \"perfect_answer\",\n",
    "                \"value\": \"100.0 (overwrites previous)\",\n",
    "                \"reason\": \"Perfect format and correct answers\"\n",
    "            })\n",
    "\n",
    "        # Store final reward and success in details\n",
    "        details[\"total_reward\"] = reward\n",
    "        details[\"success\"] = success\n",
    "\n",
    "        return reward, success, details\n",
    "\n",
    "\n",
    "    def batch_shaped_correctness_reward(\n",
    "        self,\n",
    "        tokenizer: ModelTokenizer, \n",
    "        completions: torch.Tensor, \n",
    "        answers: List[str],\n",
    "        details_report: bool = False\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[List[Dict[str, Any]]]]:\n",
    "        \"\"\"\n",
    "        Apply the shaped reward function to a batch of completions.\n",
    "        DO NOT change the signature of this function, or it will break the design pattern of how this object method is invoked in grpo_full_finetune_distributed.py.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: Tokenizer for decoding completions\n",
    "            completions: Tensor of token IDs\n",
    "            answers: List of ground truth answers\n",
    "            details_report: Whether to generate detailed diagnostic reports\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (rewards, successes, optional details list)\n",
    "        \"\"\"\n",
    "        batch_size, grpo_size, *_ = completions.shape\n",
    "        rewards = torch.zeros(batch_size, grpo_size, dtype=torch.float32)\n",
    "        successes = torch.zeros(batch_size, grpo_size, dtype=torch.float32)\n",
    "\n",
    "        # Create container for details if requested\n",
    "        details_list = [] if details_report else None\n",
    "\n",
    "        # Process each completion in the batch\n",
    "        for b in range(batch_size):\n",
    "            batch_details = [] if details_report else None\n",
    "\n",
    "            for g in range(grpo_size):\n",
    "                # Decode the completion\n",
    "                text_completion = tokenizer.decode(\n",
    "                    completions[b, g].tolist()\n",
    "                )\n",
    "\n",
    "                # Calculate reward, success, and details\n",
    "                reward, success, details = self.shaped_correctness_reward(\n",
    "                    answer=answers[b], \n",
    "                    completion=text_completion\n",
    "                )\n",
    "\n",
    "                # Store results\n",
    "                rewards[b, g] = reward\n",
    "                successes[b, g] = success\n",
    "\n",
    "                # Store details if requested\n",
    "                if details_report:\n",
    "                    # Add batch and group indices\n",
    "                    details[\"batch_idx\"] = b\n",
    "                    details[\"group_idx\"] = g\n",
    "                    batch_details.append(details)\n",
    "\n",
    "            # Add batch details to the main list\n",
    "            if details_report:\n",
    "                details_list.append(batch_details)\n",
    "\n",
    "        return rewards, successes, details_list\n",
    "\n",
    "\n",
    "    # Helper function to print a readable summary of the details\n",
    "    def print_reward_details_summary(self, details: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Print a human-readable summary of the reward details.\n",
    "\n",
    "        Args:\n",
    "            details: The details dictionary from shaped_correctness_reward\n",
    "        \"\"\"\n",
    "        print(f\"=== Reward Calculation Summary ===\")\n",
    "        print(f\"Ground Truth: Era='{details['ground_truth']['era']}', Date='{details['ground_truth']['date']}'\")\n",
    "        print(f\"Completion: {details['completion']}\")\n",
    "        print(\"\\nExtracted Tags:\")\n",
    "        print(f\"  Think: {len(details['extracted_tags']['think'])} tag(s)\")\n",
    "        print(f\"  Era: {len(details['extracted_tags']['answer_era'])} tag(s)\")\n",
    "        print(f\"  Date: {len(details['extracted_tags']['answer_date'])} tag(s)\")\n",
    "\n",
    "        print(\"\\nFormat Analysis:\")\n",
    "        if details['format_analysis'].get('has_outside_text', False):\n",
    "            print(f\"  ❌ Text outside tags: {details['format_analysis']['outside_text']}\")\n",
    "        else:\n",
    "            print(f\"  ✓ No text outside tags\")\n",
    "\n",
    "        print(\"\\nContent Analysis:\")\n",
    "        if 'era' in details['content_analysis']:\n",
    "            print(f\"  Era provided: {details['content_analysis']['era']['provided']}\")\n",
    "            match_status = \"❌ No match\"\n",
    "            if details['content_analysis'].get('era_match', {}).get('exact_match', False):\n",
    "                match_status = \"✓ Exact match\"\n",
    "            elif details['content_analysis'].get('era_match', {}).get('partial_match', False):\n",
    "                match_status = \"~ Partial match\"\n",
    "            print(f\"  Era match: {match_status}\")\n",
    "\n",
    "        if 'date' in details['content_analysis']:\n",
    "            print(f\"  Date provided: {details['content_analysis']['date']['provided']}\")\n",
    "            if 'best_match' in details['content_analysis']['date']:\n",
    "                best = details['content_analysis']['date']['best_match']\n",
    "                print(f\"  Best date: {best['value']} (diff: {best['difference']} years)\")\n",
    "\n",
    "        print(\"\\nReward Components:\")\n",
    "        for component in details['reward_components']:\n",
    "            print(f\"  {component['component']}: {component['value']} - {component['reason']}\")\n",
    "\n",
    "        print(f\"\\nTotal Reward: {details['total_reward']}\")\n",
    "        print(f\"Success: {details['success']}\")\n",
    "\n",
    "        if 'success_criteria' in details:\n",
    "            criteria = details['success_criteria']\n",
    "            print(\"\\nSuccess Criteria:\")\n",
    "            print(f\"  Format perfect: {'✓' if criteria['perfect_format'] else '❌'}\")\n",
    "            print(f\"  Era correct: {'✓' if criteria['correct_era'] else '❌'}\")\n",
    "            print(f\"  Date correct: {'✓' if criteria['correct_date'] else '❌'}\")\n",
    "\n",
    "        print(\"================================\")\n",
    "\n",
    "    def display_responses(\n",
    "        self,\n",
    "        responses: torch.Tensor,\n",
    "        tokenizer: ModelTokenizer,\n",
    "        grpo_size: int,\n",
    "        advantages: Optional[torch.Tensor] = None,\n",
    "        rewards: Optional[torch.Tensor] = None,\n",
    "        successes: Optional[torch.Tensor] = None,\n",
    "        details: Optional[List[List[Dict[str, Any]]]] = None,\n",
    "        show_n: Optional[int] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Display responses with rewards, advantages, and detailed diagnostics in a visually appealing format.\n",
    "        \n",
    "        Args:\n",
    "            responses: Tensor of token IDs\n",
    "            tokenizer: Tokenizer for decoding responses\n",
    "            grpo_size: Size of the policy optimization group\n",
    "            advantages: Optional tensor of advantages\n",
    "            rewards: Optional tensor of rewards\n",
    "            successes: Optional tensor of successes\n",
    "            details: Optional list of reward calculation details\n",
    "            show_n: Optional maximum number of responses to display\n",
    "            \n",
    "        Returns:\n",
    "            HTML string for displaying the responses\n",
    "        \"\"\"\n",
    "        batch_size = responses.shape[0]\n",
    "        \n",
    "        # Helper function to safely get values from tensors with different shapes\n",
    "        def get_item_value(tensor, batch_idx, group_idx):\n",
    "            if tensor is None:\n",
    "                return None\n",
    "            \n",
    "            if tensor.dim() == 1:\n",
    "                # Handle 1D tensor [grpo_size]\n",
    "                return tensor[group_idx].item()\n",
    "            else:\n",
    "                # Handle 2D tensor [batch_size, grpo_size]\n",
    "                return tensor[batch_idx][group_idx].item()\n",
    "        \n",
    "        html_output = \"\"\"\n",
    "        <style>\n",
    "            .response-container {\n",
    "                margin: 20px 0;\n",
    "                border: 1px solid #C4C7AC;\n",
    "                border-radius: 8px;\n",
    "                overflow: hidden;\n",
    "                box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n",
    "                font-family: 'Courier New', monospace;\n",
    "                max-width: 100%;\n",
    "            }\n",
    "            .response-header {\n",
    "                background-color: #F0EBE5;\n",
    "                padding: 10px 15px;\n",
    "                font-size: 16px;\n",
    "                font-weight: bold;\n",
    "                border-bottom: 1px solid #C4C7AC;\n",
    "                color: #4A4A67;\n",
    "                display: flex;\n",
    "                justify-content: space-between;\n",
    "                align-items: center;\n",
    "            }\n",
    "            .response-body {\n",
    "                background-color: #ffffff;\n",
    "                color: #4A4A67;\n",
    "                padding: 15px;\n",
    "                white-space: pre-wrap;\n",
    "                word-wrap: break-word;\n",
    "                line-height: 1.6;\n",
    "                font-size: 14px;\n",
    "            }\n",
    "            .think-tag {\n",
    "                color: #BE6A1A;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .answer-tag {\n",
    "                color: #2C6846;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .answer-era-tag {\n",
    "                color: #2C6846;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .answer-date-tag {\n",
    "                color: #2C6846;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .metrics-container {\n",
    "                background-color: #F0EBE5;\n",
    "                border-top: 1px solid #C4C7AC;\n",
    "                padding: 10px 15px;\n",
    "            }\n",
    "            .metric-label {\n",
    "                color: #4A4A67;\n",
    "            }\n",
    "            .metric-score {\n",
    "                font-family: monospace;\n",
    "                font-weight: bold;\n",
    "                padding: 2px 8px;\n",
    "                border-radius: 4px;\n",
    "                display: inline-block;\n",
    "                margin-right: 8px;\n",
    "            }\n",
    "            .score-high {\n",
    "                background-color: #D3EFE0;\n",
    "                color: #177350;\n",
    "            }\n",
    "            .score-medium {\n",
    "                background-color: #FCF1D6;\n",
    "                color: #BE6A1A;\n",
    "            }\n",
    "            .score-low {\n",
    "                background-color: #FAD9D8;\n",
    "                color: #C5393A;\n",
    "            }\n",
    "            .success-badge {\n",
    "                background-color: #177350;\n",
    "                color: white;\n",
    "                padding: 3px 8px;\n",
    "                border-radius: 4px;\n",
    "                font-size: 12px;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .failure-badge {\n",
    "                background-color: #C5393A;\n",
    "                color: white;\n",
    "                padding: 3px 8px;\n",
    "                border-radius: 4px;\n",
    "                font-size: 12px;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .metrics-toggle {\n",
    "                cursor: pointer;\n",
    "                color: #3F7DC9;\n",
    "                text-decoration: underline;\n",
    "                font-size: 12px;\n",
    "                margin-top: 5px;\n",
    "                display: inline-block;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .details-container {\n",
    "                display: none;\n",
    "                margin-top: 10px;\n",
    "                border-top: 1px solid #C4C7AC;\n",
    "                padding-top: 10px;\n",
    "            }\n",
    "            \n",
    "            /* Reward details styling */\n",
    "            .reward-component {\n",
    "                margin-bottom: 10px;\n",
    "                padding: 8px;\n",
    "                border-radius: 4px;\n",
    "                background-color: #f8f9fa;\n",
    "            }\n",
    "            .component-name {\n",
    "                font-weight: bold;\n",
    "                color: #4A4A67;\n",
    "            }\n",
    "            .component-value {\n",
    "                font-family: monospace;\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 3px;\n",
    "            }\n",
    "            .component-value-positive {\n",
    "                background-color: #D3EFE0;\n",
    "                color: #177350;\n",
    "            }\n",
    "            .component-value-negative {\n",
    "                background-color: #FAD9D8;\n",
    "                color: #C5393A;\n",
    "            }\n",
    "            .component-reason {\n",
    "                font-size: 0.9em;\n",
    "                color: #555;\n",
    "                margin-top: 4px;\n",
    "            }\n",
    "            .check-success {\n",
    "                color: #177350;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .check-fail {\n",
    "                color: #C5393A;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            .batch-header {\n",
    "                margin: 30px 0 10px 0;\n",
    "                padding: 5px 10px;\n",
    "                background-color: #E5E7D9;\n",
    "                border-left: 4px solid #4A4A67;\n",
    "                color: #4A4A67;\n",
    "                font-size: 18px;\n",
    "                font-weight: bold;\n",
    "            }\n",
    "        </style>\n",
    "        \n",
    "        <script>\n",
    "        function toggleDetails(batchIdx, groupIdx) {\n",
    "            var detailsId = 'details-' + batchIdx + '-' + groupIdx;\n",
    "            var details = document.getElementById(detailsId);\n",
    "            var buttonId = 'toggle-' + batchIdx + '-' + groupIdx;\n",
    "            var toggleBtn = document.getElementById(buttonId);\n",
    "            \n",
    "            if (details) {\n",
    "                if (details.style.display === 'none' || details.style.display === '') {\n",
    "                    details.style.display = 'block';\n",
    "                    if (toggleBtn) toggleBtn.innerText = 'Hide Details';\n",
    "                } else {\n",
    "                    details.style.display = 'none';\n",
    "                    if (toggleBtn) toggleBtn.innerText = 'Show Details';\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        </script>\n",
    "        \"\"\"\n",
    "        \n",
    "        if show_n is not None:\n",
    "            grpo_size = min(grpo_size, show_n)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            html_output += f'<div class=\"batch-header\">Batch #{b+1}</div>'\n",
    "            \n",
    "            for g in range(grpo_size):\n",
    "                # Decode the response\n",
    "                response_text = tokenizer.decode(responses[b, g].tolist())\n",
    "                \n",
    "                # Determine if this response succeeded\n",
    "                success_value = get_item_value(successes, b, g) if successes is not None else None\n",
    "                is_successful = success_value == 1.0 if success_value is not None else None\n",
    "                \n",
    "                # Get reward and advantage if available\n",
    "                reward_value = get_item_value(rewards, b, g) if rewards is not None else None\n",
    "                advantage_value = get_item_value(advantages, b, g) if advantages is not None else None\n",
    "                \n",
    "                # Start response container\n",
    "                html_output += f'<div class=\"response-container\">'\n",
    "                \n",
    "                # Response header\n",
    "                html_output += f'<div class=\"response-header\">'\n",
    "                html_output += f'<div>Response #{g+1}</div>'\n",
    "                \n",
    "                # Add success/fail badge if available\n",
    "                if is_successful is not None:\n",
    "                    if is_successful:\n",
    "                        html_output += f'<div class=\"success-badge\">SUCCESS</div>'\n",
    "                    else:\n",
    "                        html_output += f'<div class=\"failure-badge\">FAIL</div>'\n",
    "                \n",
    "                html_output += '</div>'  # End response header\n",
    "                \n",
    "                # Response body with tag highlighting\n",
    "                html_output += f'<div class=\"response-body\">'\n",
    "                \n",
    "                # Escape HTML but preserve line breaks\n",
    "                escaped_text = html.escape(response_text).replace('\\n', '<br>')\n",
    "                \n",
    "                # Highlight tags\n",
    "                escaped_text = re.sub(\n",
    "                    r'&lt;think&gt;(.+?)&lt;/think&gt;',\n",
    "                    r'<span class=\"think-tag\">&lt;think&gt;</span>\\1<span class=\"think-tag\">&lt;/think&gt;</span>',\n",
    "                    escaped_text,\n",
    "                    flags=re.DOTALL\n",
    "                )\n",
    "                \n",
    "                escaped_text = re.sub(\n",
    "                    r'&lt;answer_era&gt;(.+?)&lt;/answer_era&gt;',\n",
    "                    r'<span class=\"answer-era-tag\">&lt;answer_era&gt;</span>\\1<span class=\"answer-era-tag\">&lt;/answer_era&gt;</span>',\n",
    "                    escaped_text\n",
    "                )\n",
    "                \n",
    "                escaped_text = re.sub(\n",
    "                    r'&lt;answer_date&gt;(.+?)&lt;/answer_date&gt;',\n",
    "                    r'<span class=\"answer-date-tag\">&lt;answer_date&gt;</span>\\1<span class=\"answer-date-tag\">&lt;/answer_date&gt;</span>',\n",
    "                    escaped_text\n",
    "                )\n",
    "                \n",
    "                html_output += escaped_text\n",
    "                html_output += '</div>'  # End response body\n",
    "                \n",
    "                # Metrics container\n",
    "                if reward_value is not None or advantage_value is not None:\n",
    "                    html_output += f'<div class=\"metrics-container\">'\n",
    "                    \n",
    "                    # Determine score class based on reward value\n",
    "                    score_class = \"score-high\" if reward_value and reward_value >= 80 else \\\n",
    "                                \"score-medium\" if reward_value and reward_value >= 30 else \\\n",
    "                                \"score-low\"\n",
    "                    \n",
    "                    # Display reward\n",
    "                    if reward_value is not None:\n",
    "                        html_output += f'<div><strong class=\"metric-label\">Reward:</strong> <span class=\"metric-score {score_class}\">{reward_value:.1f}</span></div>'\n",
    "                    \n",
    "                    # Display advantage\n",
    "                    if advantage_value is not None:\n",
    "                        adv_class = \"score-high\" if advantage_value > 0 else \"score-low\"\n",
    "                        html_output += f'<div><strong class=\"metric-label\">Advantage:</strong> <span class=\"metric-score {adv_class}\">{advantage_value:.1f}</span></div>'\n",
    "                    \n",
    "                    # Add details toggle if available\n",
    "                    if details is not None and b < len(details) and g < len(details[b]):\n",
    "                        html_output += f'<a id=\"toggle-{b}-{g}\" class=\"metrics-toggle\" onclick=\"toggleDetails({b}, {g})\">Show Details</a>'\n",
    "                        html_output += f'<div id=\"details-{b}-{g}\" class=\"details-container\">'\n",
    "                        \n",
    "                        # Format reward details\n",
    "                        detail_data = details[b][g]\n",
    "                        \n",
    "                        # Ground truth\n",
    "                        html_output += f'<div style=\"margin-bottom: 15px;\">'\n",
    "                        html_output += f'<div><strong>Ground Truth:</strong> Era=\\'{detail_data[\"ground_truth\"][\"era\"]}\\', Date=\\'{detail_data[\"ground_truth\"][\"date\"]}\\'</div>'\n",
    "                        html_output += f'</div>'\n",
    "                        \n",
    "                        # Extracted tags\n",
    "                        html_output += f'<div style=\"margin-bottom: 15px;\">'\n",
    "                        html_output += f'<div><strong>Extracted Tags:</strong></div>'\n",
    "                        html_output += f'<ul style=\"margin-top: 5px; padding-left: 20px;\">'\n",
    "                        html_output += f'<li>Think: {len(detail_data[\"extracted_tags\"][\"think\"])} tag(s)</li>'\n",
    "                        html_output += f'<li>Era: {len(detail_data[\"extracted_tags\"][\"answer_era\"])} tag(s)</li>'\n",
    "                        html_output += f'<li>Date: {len(detail_data[\"extracted_tags\"][\"answer_date\"])} tag(s)</li>'\n",
    "                        html_output += f'</ul>'\n",
    "                        html_output += f'</div>'\n",
    "                        \n",
    "                        # Format analysis\n",
    "                        html_output += f'<div style=\"margin-bottom: 15px;\">'\n",
    "                        html_output += f'<div><strong>Format Analysis:</strong></div>'\n",
    "                        \n",
    "                        if detail_data['format_analysis'].get('has_outside_text', False):\n",
    "                            outside_text = html.escape(detail_data['format_analysis']['outside_text'])\n",
    "                            html_output += f'<div class=\"check-fail\">❌ Text outside tags: \"{outside_text}\"</div>'\n",
    "                        else:\n",
    "                            html_output += f'<div class=\"check-success\">✓ No text outside tags</div>'\n",
    "                        \n",
    "                        html_output += f'</div>'\n",
    "                        \n",
    "                        # Content analysis\n",
    "                        if 'content_analysis' in detail_data:\n",
    "                            html_output += f'<div style=\"margin-bottom: 15px;\">'\n",
    "                            html_output += f'<div><strong>Content Analysis:</strong></div>'\n",
    "                            \n",
    "                            # Era analysis\n",
    "                            if 'era' in detail_data['content_analysis']:\n",
    "                                provided_eras = ', '.join([f\"'{era}'\" for era in detail_data['content_analysis']['era']['provided']])\n",
    "                                html_output += f'<div style=\"margin-top: 5px;\"><strong>Era provided:</strong> {provided_eras}</div>'\n",
    "                                \n",
    "                                match_status = \"\"\n",
    "                                if detail_data['content_analysis'].get('era_match', {}).get('exact_match', False):\n",
    "                                    match_status = f'<span class=\"check-success\">✓ Exact match</span>'\n",
    "                                elif detail_data['content_analysis'].get('era_match', {}).get('partial_match', False):\n",
    "                                    match_status = f'<span style=\"color: #BE6A1A; font-weight: bold;\">~ Partial match</span>'\n",
    "                                else:\n",
    "                                    match_status = f'<span class=\"check-fail\">❌ No match</span>'\n",
    "                                \n",
    "                                html_output += f'<div><strong>Era match:</strong> {match_status}</div>'\n",
    "                            \n",
    "                            # Date analysis\n",
    "                            if 'date' in detail_data['content_analysis']:\n",
    "                                provided_dates = ', '.join([f\"'{date}'\" for date in detail_data['content_analysis']['date']['provided']])\n",
    "                                html_output += f'<div style=\"margin-top: 5px;\"><strong>Date provided:</strong> {provided_dates}</div>'\n",
    "                                \n",
    "                                if 'best_match' in detail_data['content_analysis']['date']:\n",
    "                                    best = detail_data['content_analysis']['date']['best_match']\n",
    "                                    diff_class = \"check-success\" if best['difference'] <= 20 else \\\n",
    "                                                (\"color: #BE6A1A; font-weight: bold;\" if best['difference'] <= 50 else \"check-fail\")\n",
    "                                    \n",
    "                                    html_output += f'<div><strong>Best date:</strong> {best[\"value\"]} <span style=\"{diff_class}\">(diff: {best[\"difference\"]} years)</span></div>'\n",
    "                            \n",
    "                            html_output += f'</div>'\n",
    "                        \n",
    "                        # Reward components table\n",
    "                        html_output += f'<div style=\"margin-bottom: 15px;\">'\n",
    "                        html_output += f'<div><strong>Reward Components:</strong></div>'\n",
    "                        html_output += f'<div style=\"margin-top: 10px;\">'\n",
    "                        \n",
    "                        for component in detail_data['reward_components']:\n",
    "                            component_name = component['component']\n",
    "                            value = component['value']\n",
    "                            reason = component['reason']\n",
    "                            \n",
    "                            # Determine CSS class based on value\n",
    "                            try:\n",
    "                                value_float = float(str(value).replace(\"(overwrites previous)\", \"\"))\n",
    "                                value_class = \"component-value-positive\" if value_float > 0 else \"component-value-negative\"\n",
    "                            except:\n",
    "                                value_class = \"\"\n",
    "                            \n",
    "                            html_output += f'<div class=\"reward-component\">'\n",
    "                            html_output += f'<div><span class=\"component-name\">{component_name}:</span> <span class=\"component-value {value_class}\">{value}</span></div>'\n",
    "                            html_output += f'<div class=\"component-reason\">{reason}</div>'\n",
    "                            html_output += f'</div>'\n",
    "                        \n",
    "                        html_output += f'</div>'\n",
    "                        html_output += f'</div>'\n",
    "                        \n",
    "                        # Success criteria\n",
    "                        if 'success_criteria' in detail_data:\n",
    "                            criteria = detail_data['success_criteria']\n",
    "                            html_output += f'<div style=\"margin-bottom: 15px;\">'\n",
    "                            html_output += f'<div><strong>Success Criteria:</strong></div>'\n",
    "                            html_output += f'<ul style=\"margin-top: 5px; padding-left: 20px;\">'\n",
    "                            \n",
    "                            html_output += f'<li><span class=\"{\"check-success\" if criteria[\"perfect_format\"] else \"check-fail\"}\">{(\"✓\" if criteria[\"perfect_format\"] else \"❌\")} Format perfect</span></li>'\n",
    "                            html_output += f'<li><span class=\"{\"check-success\" if criteria[\"correct_era\"] else \"check-fail\"}\">{(\"✓\" if criteria[\"correct_era\"] else \"❌\")} Era correct</span></li>'\n",
    "                            html_output += f'<li><span class=\"{\"check-success\" if criteria[\"correct_date\"] else \"check-fail\"}\">{(\"✓\" if criteria[\"correct_date\"] else \"❌\")} Date correct</span></li>'\n",
    "                            \n",
    "                            html_output += f'</ul>'\n",
    "                            html_output += f'</div>'\n",
    "                        \n",
    "                        # Total reward summary\n",
    "                        html_output += f'<div style=\"margin-top: 15px; padding-top: 10px; border-top: 1px solid #C4C7AC;\">'\n",
    "                        html_output += f'<div><strong>Total Reward:</strong> <span class=\"{\"score-high\" if detail_data[\"total_reward\"] > 0 else \"score-low\"}\">{detail_data[\"total_reward\"]}</span></div>'\n",
    "                        html_output += f'<div><strong>Success:</strong> <span class=\"{\"score-high\" if detail_data[\"success\"] > 0 else \"score-low\"}\">{detail_data[\"success\"]}</span></div>'\n",
    "                        html_output += f'</div>'\n",
    "                        \n",
    "                        html_output += f'</div>'  # End details container\n",
    "                    \n",
    "                    html_output += f'</div>'  # End metrics container\n",
    "                \n",
    "                html_output += f'</div>'  # End response container\n",
    "        \n",
    "        return html_output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rewards_v0 import RewardServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RewardServer()\n",
    "rewards, successes, details = rs.batch_shaped_correctness_reward(     \n",
    "  tokenizer=tokenizer,      \n",
    "  completions=responses,      \n",
    "  answers=_answers,\n",
    "  details_report=True  # Enables detailed diagnostics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx = 0\n",
    "group_member_idx = 0\n",
    "rs.print_reward_details_summary(details[batch_idx][group_member_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages = (rewards - rewards.mean(1, keepdim=True)) / (\n",
    "    rewards.std(1, keepdim=True) + 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\n",
    "    rs.display_responses(\n",
    "        responses,\n",
    "        tokenizer, \n",
    "        grpo_size, \n",
    "        advantages=advantages, \n",
    "        rewards=rewards, \n",
    "        successes=successes,\n",
    "        details=details # OPTIONAL\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
